{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNoownvGOV5rK5iFw4lZDoN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Uncertainty Quantification in Deep Learning (Regression)\n","\n","Rubén Martínez Cantín, Javier Civera\n","\n","rmcantin@unizar.es\n","\n","University of Zaragoza\n","\n","\n","\n","In this part, we will analyze the performance of scalable Bayesian networks for regression problems. Specifically, we will use the Boston dataset\n","\n","Do the following tasks:\n","* Run the code and make sure you understand all of the cells.\n","* Train a neural network (e.g., the one defined in the notebook) and report some regression metrics (e.g., mean absolute error)\n","* Train a Bayesian neural network with aleatoric uncertainty and report metrics for the error and the uncertainty. For the uncertainty, note that you have a function `ause(abs error,y var)` in the Colab notebook.\n","* Train a Bayesian neural network with aleatoric and epistemic uncertainty, the latest using an ensemble of N networks. Report error metrics and AUSE and its dependence with the number of models in the ensemble. The global uncertainty, combining epistemic and aleatoric, can be approximated by:\n","\n","$$  \\sigma^2 = \\underbrace{\\frac{1}{M} \\sum_{i=1}^M \\left(y_i - \\bar{y}\\right)^2}_{\\text{epistemic}} + \\underbrace{\\frac{1}{M} \\sum_{i=1}^M \\sigma_{y,i}^2}_{\\text{aleatoric}}$$\n","\n","where $\\bar{y} = \\frac{1}{M} \\sum_{i=1}^M y_i$ and $(y_i, \\sigma_{y,i}^2)$\n","correspond to the mean and the variance predicted by the i-th network of the\n","ensemble.\n","\n","* Train a Bayesian neural network with aleatoric and epistemic uncertainty, this time using MC Dropout. For that, add Dropout layers to your model using:\n","```\n","x = F.Dropout(x, p=dropout_p, training=True)\n","```\n","instead of\n","```\n","x = self.drop(x)\n","```\n","The difference is that `self.drop` use a `nn.Module` which already defines if the network is training or testing (and deactivates dropout for testing). Using the functional version `F.Dropout` we can force the layer to be always active by tricking the layer to think to it is always training.\n","\n","Analyze its performance, the dependence of the errors and uncertainty calibration with the number of forward passes, and compare it against ensembles.\n","\n","* Are the test errors consistent with the predicted uncertainties? The estimation of the uncertainty is under- or over-confident?\n","* Generate an out-of-distribution test sample and check its high predicted uncertainty.\n","\n","* Run the code for Laplace approximation with different configurationas and evaluate the performance.\n","\n","If you want, you can make several copies of this notebook and train each method in a different colab (Monte Carlo, Ensembles and Laplace)."],"metadata":{"id":"_pK6vbMVlGOo"}},{"cell_type":"code","source":["#@title Install Dependencies\n","#@markdown This is only required for laplace. You can remove this for MC-dropout and ensembles\n","!pip install laplace-torch"],"metadata":{"id":"SJ3cZCa1faT3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KukdI9CRk27s"},"outputs":[],"source":["#@title Import Dependencies\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import accuracy_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.distributions as dists\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","\n","#@markdown Again, the last import is only required for Laplace. Remove it if you are only doing MC or ensembles.\n","from laplace import Laplace"]},{"cell_type":"code","source":["#@title Define Hyperparameters\n","#@markdown Hyperparameters for the different algoritms, model, etc. Feel free to add more if needed.\n","#@markdown\n","#@markdown **TODO:** The hyperparameters are NOT optimal. Change them as need it.\n","#@markdown Also, consider reducing the number of epochs and ensembles for development.\n","\n","\n","num_ensembles = 32 #@param {type: \"integer\"}\n","num_epochs = 200  #@param {type: \"integer\"} # number of times which the entire dataset is passed throughout the model\n","batch_size = 100 #@param {type: \"integer\"} # the size of input data took for one iteration\n","lr = 1e-4 #@param {type: \"number\"} # size of step\n","weight_decay = 0.00005 #@param {type: \"number\"}\n","dropout_p = 0.5 #@param {type: \"number\"}\n","\n","n_epochs_Laplace = 10 #@param {type: \"integer\"} # epochs for Laplace hyperparameters\n","\n","use_gpu = True #@param {type: \"boolean\"}\n","gpu_id = 0 #@param {type: \"integer\"}\n","\n","if torch.cuda.is_available() and use_gpu:\n","    device = torch.device(\"cuda:\" + str(gpu_id))\n","    print(\"Using GPU id {}\".format(gpu_id))\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not detected. Defaulting to CPU.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kgq3ghaff4aj","executionInfo":{"status":"ok","timestamp":1683303172299,"user_tz":-120,"elapsed":30,"user":{"displayName":"Ruben Martinez-Cantin","userId":"11446784088370650030"}},"outputId":"7a5000e7-1d11-45d1-d998-c23df662d952"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU id 0\n"]}]},{"cell_type":"markdown","source":["# Loading the dataset\n","\n","In the next block, we are going to import the California housing dataset from sklearn. If you want, you can use any other regression datasets. For example, you can try any of the following datasets from UCI:\n","\n","* Wine quality dataset:\n","https://archive-beta.ics.uci.edu/dataset/186/wine+quality\n","```\n","import pandas as pd\n","!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" --no-check-certificate\n","data = pd.read_csv('winequality-red.csv', header=1, delimiter=';').values\n","data = data[np.random.permutation(np.arange(len(data)))]\n","```\n","\n","* Concrete compression:\n","https://archive-beta.ics.uci.edu/dataset/165/concrete+compressive+strength\n","```\n","import pandas as pd\n","!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\" --no-check-certificate\n","data = pd.read_excel('Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n","data = data[np.random.permutation(np.arange(len(data)))]\n","```\n","\n","* Combined cycle power plant:\n","https://archive-beta.ics.uci.edu/dataset/294/combined+cycle+power+plant\n","```\n","import pandas as pd\n","!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\" --no-check-certificate\n","zipped = zipfile.ZipFile(\"CCPP.zip\")\n","data = pd.read_excel(zipped.open('CCPP/Folds5x2_pp.xlsx'), header=0, delimiter=\"\\t\").values\n","```\n","\n","* Yacht hydrodynamics: https://archive-beta.ics.uci.edu/dataset/243/yacht+hydrodynamics\n","```\n","import pandas as pd\n","!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\" --no-check-certificate\n","data = pd.read_csv('yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n","data = data[np.random.permutation(np.arange(len(data)))]\n","```\n","\n","\n","**Important:** After loading any of the previous datasets, you should include the following lines:\n","```\n","in_dim = data.shape[1] - 1\n","\n","X = torch.Tensor(data[:, :in_dim])\n","Y = torch.Tensor(data[:, in_dim:])\n","```"],"metadata":{"id":"ybaFhhQu-g66"}},{"cell_type":"code","source":["#@title Loading California housing.\n","#@markdown Remove if you load other dataset.\n","from sklearn.datasets import fetch_california_housing\n","\n","data = fetch_california_housing()\n","\n","X = torch.Tensor(data.data)\n","Y = torch.Tensor(data.target)"],"metadata":{"id":"J4FsCZtfgd-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Data preprocessing\n","#@markdown We are going to build the train/test split and preprocess all the data\n","#@markdown (normalization, building data loaders, etc.)\n","\n","import torch.utils.data as data_utils\n","from sklearn.preprocessing import StandardScaler\n","\n","#import sklearn model selection package to split data into train and test models\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state = 42)\n","\n","# We rescale de data to avoid numerical problems.\n","# The scaling factor is computed only with training data to avoid transfering\n","# knowledge to test data, but applied in both datasets.\n","scaler = StandardScaler().fit(x_train)\n","x_train = scaler.transform(x_train)\n","x_test = scaler.transform(x_test)\n","\n","train_dataset = data_utils.TensorDataset(torch.Tensor(x_train),torch.Tensor(y_train))\n","train_gen = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n","\n","test_dataset = data_utils.TensorDataset(torch.Tensor(x_test),torch.Tensor(y_test))\n","test_gen = data_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle = False)"],"metadata":{"id":"UQWE6UVGrwq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define model class\n","#@markdown We have defined two models. One traditional regression model where the\n","#@markdown output is predicted value (assuming homoscedastic noise) and other with\n","#@markdown two-heads where we both predict the value and the aleatoric uncertainty\n","#@markdown (heteroscedastic noise).\n","#@markdown\n","#@markdown **TODO:** This model is not optimal for any of the datasets. You should try different models.\n","\n","\n","input_size = x_train.shape[1]\n","\n","class HomoMLP(nn.Module):\n","  def __init__(self, input_size):\n","    super(HomoMLP,self).__init__()\n","    self.fc1 = nn.Linear(input_size, 128)\n","    self.fc2 = nn.Linear(128, 64)\n","    self.fc3 = nn.Linear(64, 32)\n","    self.fc4 = nn.Linear(32, 1)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self,x):\n","    x = self.relu(self.fc1(x))\n","    x = self.relu(self.fc2(x))\n","    x = self.relu(self.fc3(x))\n","    return self.fc4(x)\n","\n","class HeteroMLP(HomoMLP):\n","  def __init__(self, input_size):\n","    super(HeteroMLP,self).__init__(input_size)\n","    self.fc4 = nn.Linear(32, 2)"],"metadata":{"id":"LJ1F5p4Ugr70"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Metrics\n","\n","In the following section, we have defined some metrics for training or evaluation that you can use:\n","\n","* **Negative log likelihood (NLL)**: Asumes `y_pred[:,0]` is the mean $\\mu$ and `y_pred[:,1]` is the log of the variance $\\log \\sigma^2$; `y_true` is the ground truth values.  You can also use\n","\n","```\n","-dists.Normal(y_pred[:,0], torch.exp(y_pred[:,1])).log_prob(y_true).mean()\n","```\n","\n","* **Mean absolute error (MAE)**: Asumes `y_pred[:,0]` is the mean $\\mu$ and `y_pred[:,1]` is the log of the variance $\\log \\sigma^2$; `y_true` is the ground truth values.  You can also use:\n","\n","```\n","mae = nn.L1Loss()\n","mae(y_pred[:,0], y_true)\n","```\n","\n","* **AUSE**: Computes the ause given a vector/array with the absolute errors and the variances. The arrays can be numpy arrays or pytorch tensors. It also **plots** the figure with the sparsification curves. Feel free to modify the plot to your preferences.\n","\n","* **Root mean square error (RMSE)**: That function is not implemented, but you can easily do it based on MAE. You can also use the MSE loss from pytorch:\n","\n","```\n","mse = nn.MSELoss()\n","torch.sqrt(mse(y_pred[:,0], y_true))\n","```\n"],"metadata":{"id":"qHoKoEITJuHN"}},{"cell_type":"code","source":["#@title Metrics and losses\n","\n","def neg_log_likelihood(y_pred, y_true):\n","  '''Computes the negative log-likelihood for training\n","  '''\n","  y_mean = y_pred[:,0]\n","  y_logvar = y_pred[:,1]\n","  return torch.mean(0.5*torch.square(y_true - y_mean)/torch.exp(y_logvar) + 0.5 * y_logvar)\n","\n","def mean_abs_error(y_pred, y_true):\n","  '''Outputs the mae, for inspecting the training\n","  '''\n","  y_mean = y_pred[:,0]\n","  return torch.mean(torch.abs(y_true - y_mean))\n","\n","\n","def ause(abs_error, y_var, plot_curves=True):\n","  '''Computes the AUSE error based on the variance and absolute error.\n","     It also plots the figure with the sparsification curves.\n","     Feel free to modify the plot to your preferences.\n","  '''\n","  if torch.is_tensor(abs_error):\n","    abs_error = abs_error.detach().cpu().numpy()\n","  if torch.is_tensor(y_var):\n","    y_var = y_var.detach().cpu().numpy()\n","\n","  abs_error /= np.sum(abs_error)\n","  idx_errors = np.argsort(abs_error)[::-1]\n","  idx_variances = np.argsort(y_var)[::-1]\n","  scurve_errors = np.array([1])\n","  scurve_variances = np.array([1])\n","  abs_error_sorted = abs_error[idx_errors]\n","  variances_sorted = abs_error[idx_variances]\n","\n","  for _ in range(len(abs_error)-1):\n","    abs_error_sorted = np.delete(abs_error_sorted,0)\n","    scurve_errors = np.append(scurve_errors,np.sum(abs_error_sorted))\n","    variances_sorted = np.delete(variances_sorted,0)\n","    scurve_variances = np.append(scurve_variances,np.sum(variances_sorted))\n","\n","  percentages = (np.arange(len(scurve_errors))+1) / len(scurve_errors)\n","\n","  if plot_curves:\n","    plt.plot(percentages, scurve_errors)\n","    plt.plot(percentages, scurve_variances)\n","    plt.title('Sparsification error curves')\n","    plt.ylabel('mae')\n","    plt.xlabel('ratio of samples removed')\n","    plt.legend(['oracle', 'model'], loc='lower left')\n","    plt.show()\n","\n","  integral_errors = np.trapz(scurve_errors, percentages)\n","  integral_variances = np.trapz(scurve_variances, percentages)\n","\n","  return np.abs(integral_errors - integral_variances)"],"metadata":{"id":"4wCSPY-Nuhqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Train and evaluate model\n","#@markdown Functions for training and evaluating the model. Note that the training\n","#@markdown function is set to evaluate the model after each epoch. You can change\n","#@markdown that behaviour.\n","\n","def eval_step(model, test_gen, loss_function):\n","    model.eval()\n","    train_acc = 0\n","    val_loss = 0\n","    with torch.no_grad():\n","      for i ,(inputs,labels) in enumerate(test_gen):\n","        inputs = inputs.to(device)\n","\n","        # WARNING: Depending on the dataset, you might need to squeeze\n","        # or unsqueeze the labels\n","        labels = labels.to(device).unsqueeze(1)\n","\n","        outputs = model(inputs)\n","        loss_val = loss_function(outputs, labels)\n","\n","        # Calculate and accumulate accuracy metric across all batches\n","        train_acc += mean_abs_error(outputs, labels)\n","\n","    val_loss = loss_val.item()\n","    return val_loss, train_acc/len(test_gen)\n","\n","def train_model(model, train_gen, test_gen, loss_function, optimizer):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for i ,(inputs,labels) in enumerate(train_gen):\n","            inputs = inputs.to(device)\n","            # WARNING: Depending on the dataset, you might need to squeeze\n","            # or unsqueeze the labels\n","            labels = labels.to(device).unsqueeze(1)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = loss_function(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","        val_loss, train_acc = eval_step(model, test_gen, loss_function)\n","        mae = mean_abs_error(outputs, labels)\n","        print('Epoch [%d/%d], Loss: %.4f, MAE: %.4f, Val loss: %.4f, Val MAE: %.4f'\n","                      %(epoch+1, num_epochs, loss.item(), mae, val_loss, train_acc))"],"metadata":{"id":"RDL_AbzAvgG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Training a base model (homoscedastic)\n","#@markdown This a pure deterministic model, assuming homoscedastic noise.\n","\n","homo_model = HomoMLP(input_size).to(device)\n","loss_function = nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.Adam(homo_model.parameters(), lr=lr, weight_decay=weight_decay)\n","train_model(homo_model, train_gen, test_gen, loss_function, optimizer)"],"metadata":{"id":"A36Lo6Njpfyz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Training a heteroscedastic model\n","#@markdown This a pure deterministic model, assuming heteroscedastic noise.\n","\n","hetero_model = HeteroMLP(input_size).to(device)\n","loss_function = neg_log_likelihood\n","optimizer = torch.optim.Adam(hetero_model.parameters(), lr=lr, weight_decay=weight_decay)\n","train_model(hetero_model, train_gen, test_gen, loss_function, optimizer)"],"metadata":{"id":"N1X7p2eljtfx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Training deep ensembles (with heteroscedastic noise)\n","\n","#@markdown This is the same code as before for deep ensembles.\n","#@markdown\n","#@markdown Because all models inherit from `nn.Module`, pytorch already perform\n","#@markdown random initialization for us each time we create a new model.\n","#@markdown You can train ensembles with different base models (homoscedastic,\n","#@markdown different architectures, etc.)\n","\n","models = []\n","\n","for im in range(num_ensembles):\n","    print('Training model %d out of %d...........................................................' % (im+1, num_ensembles))\n","    loss_function = neg_log_likelihood\n","    hetero_model = HeteroMLP(input_size).to(device)\n","    optimizer = torch.optim.Adam(hetero_model.parameters(), lr=lr, weight_decay=weight_decay)\n","    train_model(hetero_model, train_gen, test_gen, loss_function, optimizer)\n","    models.append(hetero_model)"],"metadata":{"id":"JNxrj9H4xtwq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Laplace model\n","\n","In this part of the lab, we are going to use the Laplace approximation to approximate the posterior and predictive posterior distributions.\n","\n","For the implemenation, we are going to use the Laplace-redux library\n","* Code: https://github.com/AlexImmer/Laplace\n","* Paper: https://arxiv.org/abs/2106.14806\n","* Docs: https://aleximmer.github.io/Laplace/\n","\n","We assume that we have already a pretrained model. For simplicity, we are going to use the **homoscedastic model** and optimize a single **noise variance** value separately for all the inputs $\\sigma_n^2$. Furthermore, we will also optimize the prior distribution, which we assume to be in ther form $\\mathcal{N}(0, \\lambda I)$, where $\\lambda$ is called the **prior precision**.\n","\n","You can play with the different configurations of weights and Hessian structure. See fig. 2 in the paper for a quick overview of the options. You can try using a less expensive approximation and optimize the hyperparamenters for more epochs.\n","\n","**Note:** The hyperparameters for noise variance and prior precision are optimized in log-space $(\\log \\sigma_n^2, \\log \\lambda)$ because it is numerically more stable and produce better results.\n","\n","We provide some code to evaluate the Laplace model as an example, but you should use your own metrics and analysis."],"metadata":{"id":"4GphO-5wPpCs"}},{"cell_type":"code","source":["#@title Fitting Laplace model\n","#@markdown We are going to use a pretrained model.\n","#@markdown See figure 2 of the paper on how to configure the Laplace approximation.\n","#@markdown Try changing the parameters of the `Laplace(...)` model (weights, Hessian...)\n","\n","# Configure and fit the model\n","print('Fitting post-hoc Laplace model....................')\n","la = Laplace(\n","    homo_model,\n","    'regression',\n","    subset_of_weights='all',\n","    hessian_structure='full'\n","    )\n","\n","la.fit(train_gen)\n","print('Done')\n","\n","log_prior, log_sigma = torch.ones(1, requires_grad=True), torch.ones(1, requires_grad=True)\n","hyper_optimizer = torch.optim.Adam([log_prior, log_sigma], lr=1e-1)\n","for i in range(n_epochs_Laplace):\n","    hyper_optimizer.zero_grad()\n","    neg_marglik = - la.log_marginal_likelihood(log_prior.exp(), log_sigma.exp())\n","    neg_marglik.backward()\n","    hyper_optimizer.step()\n","\n","    print('Epoch [%d/%d], Prior: %.4f, Sigma: %.4f'\n","                      %(i+1, n_epochs_Laplace, log_prior.exp(), log_sigma.exp()))"],"metadata":{"id":"w2-sWxLnibaz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Evaluate Laplace model\n","#@markdown We compare the Laplace model to the homoscedastic and heteroscedastic\n","#@markdown deterministic models.\n","#@markdown We show some metrics as an example. You should do your own analysis.\n","\n","def predict(dataloader, model, bayes=None):\n","    py = []\n","    pstd = []\n","    pygt = []\n","\n","    for x, y_real in dataloader:\n","        if bayes == 'Laplace':\n","            mu, var = model(x.to(device), pred_type='glm')\n","            y_pred = mu.squeeze().detach()\n","            y_var_epistemic = var.squeeze().detach()\n","            y_var_aleatoric = model.sigma_noise.item() ** 2\n","            y_pred_std = torch.sqrt(y_var_epistemic + y_var_aleatoric)\n","            pstd.append(y_pred_std)\n","        elif bayes == 'Heteroscedastic':\n","            output = model(x.to(device))\n","            y_pred = output[:,0].squeeze().detach()\n","            y_pred_std = torch.sqrt(torch.exp(output[:,1])).squeeze().detach()\n","            pstd.append(y_pred_std)\n","        else:\n","            y_pred = model(x.to(device)).squeeze().detach()\n","\n","        pygt.append(y_real)\n","        py.append(y_pred)\n","\n","    if bayes is not None:\n","        return torch.cat(pygt).cpu(), torch.cat(py).cpu(), torch.cat(pstd).cpu()\n","    else:\n","        return torch.cat(pygt).cpu(), torch.cat(py).cpu()\n","\n","# Use torch.no_grad to call the losses as pure metrics without worrying about computing extra gradients for backpropagation.\n","with torch.no_grad():\n","    y_test, y_predMAP = predict(test_gen, homo_model)\n","    y_test, y_predHetero, y_pred_stdHetero = predict(test_gen, hetero_model, bayes='Heteroscedastic')\n","    y_test, y_pred, y_pred_std = predict(test_gen, la, bayes='Laplace')\n","\n","\n","    mse = nn.MSELoss()\n","    mae = nn.L1Loss()\n","    print(f'[MAP] MSE: {mse(y_predMAP, y_test):.3}, MAE: {mae(y_predMAP, y_test):.3}')\n","\n","    nll = -dists.Normal(y_predHetero, y_pred_stdHetero).log_prob(y_test).mean()\n","    print(f'[LAP] NLL: {nll:.3}, MSE: {mse(y_pred, y_test):.3}, MAE: {mae(y_pred, y_test):.3}')\n","\n","    nll = -dists.Normal(y_pred, y_pred_std).log_prob(y_test).mean()\n","    print(f'[LAP] NLL: {nll:.3}, MSE: {mse(y_pred, y_test):.3}, MAE: {mae(y_pred, y_test):.3}')\n"],"metadata":{"id":"XxKqQLQp3pF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Histogram of the distribution of prectictions with respect to the true value\n","fig,ax = plt.subplots(3,1)\n","ax[0].hist(y_predMAP-y_test, bins=50, range=(-5,5))\n","ax[1].hist(y_predHetero-y_test, bins=50,range=(-5,5))\n","ax[2].hist(y_pred-y_test, bins=50,range=(-5,5))"],"metadata":{"id":"Ups1zQ7nuoyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#AUSE based on the mean absolute error (MAE). You can also use the MSE o RMSE instead of the MAE.\n","abs_error = torch.abs(y_pred-y_test).detach().cpu().numpy()\n","y_var = (y_pred_std**2).detach().cpu().numpy()\n","print(\"AUSE:\", ause(abs_error, y_var))"],"metadata":{"id":"waBMGR5Jlx6V"},"execution_count":null,"outputs":[]}]}