{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TKkIMxBKvRqNYOBhiAfhHrH270YslS6y","timestamp":1621006078323},{"file_id":"1TTtW67-nMw8vN5uBvqLs4MmY3DwZ859Q","timestamp":1620319082483},{"file_id":"1_TS09TEkdKWVIKj4YWSkU0f3HhPE3SzZ","timestamp":1620123384495},{"file_id":"1D9rRLYyzXp-CQrCJZ2Bt7PQK2le0BZqK","timestamp":1619884931092}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OXeFdzJxmyfF"},"source":["# Deep Q-Networks\n","\n","**Applications of Deep Learning, University of Zaragoza, Ruben Martinez-Cantin**\n","\n","*This assigment is based on the UC Berkeley course CS 285: Deep Reinforcement Learning by Sergei Levine.*\n","\n","This assignment requires you to implement and evaluate Q-learning with convolutional neural networks for playing Atari games. The Q-learning algorithm was covered in lecture, and you will be provided with starter code. This assignment will be faster to run on a GPU, though it is possible to complete on a CPU as well. We recommend using the Colab option if you do not have a GPU available to you. Please start early!\n","\n","In this assignment, we will also use external code directly pulled from Github. If you want to look that code (for example: to check how the Replay Buffer works) you can go to the github url or, after cloning the repository, you can directly open the files with the file explorer from colab.\n","\n","# 1. Implementation\n","\n","The first phase of the assignment is to implement a working version of Q-learning. The default code will run the `Ms. Pac-Man` game with reasonable hyperparameter settings. Look for the `#TODO` markers in the files listed above for detailed implementation instructions. You may want to look inside `infrastructure/dqn utils.py` (in Github or local folder) to understand how the (memory-optimized) replay bu\n","er works, but you will not need to modify it.\n","\n","Once you implement Q-learning, you might try different extensions (like double DQN) or change the hyperparameters, neural network architectures, and the game.\n","\n","To determine if your implementation of Q-learning is correct, you should run it with the default hyperparameters on the `Ms. Pac-Man` game for 1 million steps. Our reference solution gets a return of 1500 in this timeframe. On Colab, this will take roughly 3 GPU hours. If it takes much longer than that, there may be a bug in your implementation.\n","To accelerate debugging, you should try on `LunarLander-v3` first, which trains your agent to play Lunar Lander, a 1979 arcade game (also made by Atari) that has been implemented in OpenAI Gym. Our reference solution\n","with the default hyperparameters achieves around 150 reward after 350k timesteps, but there is considerable variation between runs and without the double-Q trick the average return often decreases after reaching 150.\n","\n","# 2. Evaluation\n","* **Basic Q-learning. (DQN)** Your code using basic DQN (without double) should be able to solve `MsPacman-v0`. As a performance measure, you can plot the average per-epoch reward as well as the best mean reward vs the number of time steps. These quantities are already computed and printed. They are also logged to the data folder, and can be visualized using Tensorboard as in previous assignments. You can extract the values for plotting using the cell at the end of the notebook, replacing the name of the data folder. You should\n","not need to modify the default hyperparameters in order to obtain good performance, but if you modify any of the parameters, you need to report any change in the plot. Given the GPU limitations on colab, you may report results on `LunarLander-v3`, which should take about 30 minutes. As a middle ground, you can train `Breakout` for 500k-1M timesteps. Average reward should increase more or less linearly after 150-200k timesteps.\n","\n","* **Double Q-learning (DDQN).** Use the double estimator to improve the accuracy of your learned Q values. This amounts to using the online Q network (instead of the target Q network) to select the best action when computing target values. Compare the performance of DDQN to vanilla DQN. Since there is considerable variance between runs, you must run at least three random seeds for both DQN and DDQN. You should use `LunarLander-v3` for this question. Make a single graph that averages the performance across three runs for both DQN and double DQN. You can extract the values for plotting using the cell at the end of the notebook, replacing the name of the data folder.\n","\n","* (**Optionals**) Now you can extend the assignment in multiple ways. Note that **the previous part should already give you a grade >9** if the implementation is corrent and the report is accurate.\n","  * You can experiment with the **hyperparameters**. For that, you can use shorter learning times by reducing the number of timesteps, although that limits the information on the performance on the hyperparameters. You can plot the runs with different parameters in the same graph for comparison.\n","Examples include: learning rates, neural network architecture, exploration schedule or exploration rule (e.g. you may implement an alternative to $\\epsilon$-greedy), etc. Be efficient in experimentation and report. You can combine and reuse previous runs, comparisons and discussions for this.\n","  * As a specific architecture to try, you can implement the **dueling architecture**. Note that, for the Q-values, you should combine the previous layers using equation (9) from the [paper](https://arxiv.org/pdf/1511.06581.pdf).\n","  $$\n","  Q(s,a) = V(s) + \\left(A(s,a) - \\sum_{a'} A(s,a')\\right)\n","  $$\n","  In this case, you might change only the fully connected layers, leaving the convolutional layers as defined by default.\n","  * You can also try experimenting with the hyperparameters and/or architecture of the **actor-critic lab** instead.\n","\n","#3. Submitting the code and experiment runs\n","\n","You need to submit a zip file with the code (.py or .ipynb) and the data generated in the runs.\n","\n","If you submit the ipynb file, you can replace the visualization and tensorboard boxes for text and figures briefly explainin the results (example: average expected reward in multiple runs with different seeds...).\n","\n","If you prefer, you can submit the results (text and figures) in a separated PDF instead.\n","\n","**Note:** Ideally, you should run everything for multiple seeds and see the average outcomes, but for the longer experiments, like Ms-Pacman, you can just run it once or twice to save time."]},{"cell_type":"code","source":["#@title install dependencies\n","# remove ` > /dev/null 2>&1` to see what is going on under the hood\n","!apt update > /dev/null 2>&1\n","!apt install -y --no-install-recommends \\\n","        swig \\\n","        xvfb \\\n","        python3-opengl \\\n","        ffmpeg > /dev/null 2>&1"],"metadata":{"cellView":"form","id":"2BjO_k3J3R1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"os8KvMffA1bO","cellView":"form"},"source":["#@title install Python dependencies\n","# remove ` > /dev/null 2>&1` to see what is going on under the hood\n","%pip install swig\n","%pip install gym[box2d,accept-rom-license,atari]==0.25.2 \\\n","  tensorboardX==2.5.1 \\\n","  pyvirtualdisplay==3.0 \\\n","  opencv-python==4.6.0.66 > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkxarnnfWbL2","cellView":"form"},"source":["#@title imports (torch, numpy, gym, pybullet...)\n","import numpy as np\n","import time\n","import copy\n","import abc\n","import itertools\n","import pickle\n","import os\n","\n","from collections import OrderedDict\n","from typing import Union\n","\n","import torch\n","from torch import nn\n","from torch import distributions\n","from torch import optim\n","from torch.nn import utils\n","from tensorboardX import SummaryWriter\n","\n","import gym\n","import gym.spaces\n","from gym import wrappers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gcjq-gmkRxuE","cellView":"form"},"source":["#@title clone project repo\n","#@markdown This include some functions and utils used in DQN\n","\n","#@markdown You can check the code in https://github.com/rmcantin/dqn_project.git\n","%cd /content/\n","\n","!git clone https://github.com/rmcantin/dqn_project.git\n","\n","%cd dqn_project\n","%pip install -e ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OjiUYyJAR_h","cellView":"form"},"source":["#@title code to display animations\n","from gym.wrappers import RecordVideo\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","\n","## modified from https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=TCelFzWY9MBI\n","\n","def show_video():\n","  mp4list = glob.glob('/content/video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else:\n","    print(\"Could not find video\")\n","\n","\n","def wrap_env(env):\n","  env = RecordVideo(env, '/content/video')\n","  return env\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HgeVEkRSAZvd","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653314664942,"user_tz":-120,"elapsed":540,"user":{"displayName":"Ruben Martinez-Cantin","userId":"11446784088370650030"}},"outputId":"3629d83b-b03d-4504-8ce9-58770cece1d6"},"source":["#@title set up virtual display\n","from pyvirtualdisplay import Display\n","\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7fbbc3ff6f10>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"6ZZtPkufAcmt","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"status":"ok","timestamp":1653314666205,"user_tz":-120,"elapsed":1268,"user":{"displayName":"Ruben Martinez-Cantin","userId":"11446784088370650030"}},"outputId":"0f9d27cd-dfac-4b2c-e5fa-fc6f46b574a8"},"source":["#@title test virtual display\n","\n","#@markdown If you see a video of PacMan, setup is complete!\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","\n","env = wrap_env(gym.make(\"MsPacman-v0\", render_mode=\"rgb_array\"))\n","\n","observation = env.reset()\n","for i in range(10):\n","    env.render()\n","    obs, rew, term, _ = env.step(env.action_space.sample() )\n","    if term:\n","      break;\n","\n","env.close()\n","print('Loading video...')\n","show_video()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading video...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n","             </video>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"cellView":"form","id":"T5P3pN6i5lus"},"source":["#@title set-up GPU if available\n","\n","ptu_device = None\n","\n","def ptu_init_gpu(use_gpu=True, gpu_id=0):\n","    global ptu_device\n","    if torch.cuda.is_available() and use_gpu:\n","        ptu_device = torch.device(\"cuda:\" + str(gpu_id))\n","        print(\"Using GPU id {}\".format(gpu_id))\n","    else:\n","        ptu_device = torch.device(\"cpu\")\n","        print(\"GPU not detected. Defaulting to CPU.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s2gFSGgaTAjD","cellView":"form"},"source":["#@title Other imports\n","from cs285.infrastructure import pytorch_util as ptu\n","from cs285.infrastructure.dqn_utils import MemoryOptimizedReplayBuffer\n","from cs285.infrastructure.rl_trainer import RL_Trainer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIF8eEiJm2lZ"},"source":["#@title Hyperparameters and networks for the different experiments\n","#\n","from cs285.infrastructure.dqn_utils import PiecewiseSchedule, OptimizerSpec, PreprocessAtari, Flatten\n","from cs285.infrastructure.atari_wrappers import wrap_deepmind\n","\n","def get_env_kwargs(env_name):\n","    if env_name in ['MsPacman-v0', 'PongNoFrameskip-v4', 'BreakoutDeterministic-v4']:\n","        kwargs = {\n","            'learning_starts': 50000,\n","            'target_update_freq': 10000,\n","            'replay_buffer_size': int(1e6),\n","            'num_timesteps': int(1e6), # if you cand, try int(2e8),\n","            'q_func': create_atari_q_network,\n","            'learning_freq': 4,\n","            'grad_norm_clipping': 10,\n","            'input_shape': (84, 84, 4),\n","            'env_wrappers': wrap_deepmind,\n","            'frame_history_len': 4,\n","            'gamma': 0.99,\n","        }\n","        kwargs['optimizer_spec'] = atari_optimizer(kwargs['num_timesteps'])\n","        kwargs['exploration_schedule'] = atari_exploration_schedule(kwargs['num_timesteps'])\n","\n","    elif env_name == 'LunarLander-v3':\n","        def lunar_empty_wrapper(env):\n","            return env\n","        kwargs = {\n","            'optimizer_spec': lander_optimizer(),\n","            'q_func': create_lander_q_network,\n","            'replay_buffer_size': 50000,\n","            'batch_size': 32,\n","            'gamma': 1.00,\n","            'learning_starts': 1000,\n","            'learning_freq': 1,\n","            'frame_history_len': 1,\n","            'target_update_freq': 3000,\n","            'grad_norm_clipping': 10,\n","            'lander': True,\n","            'num_timesteps': 500000,\n","            'env_wrappers': lunar_empty_wrapper\n","        }\n","        kwargs['exploration_schedule'] = lander_exploration_schedule(kwargs['num_timesteps'])\n","\n","    else:\n","        raise NotImplementedError\n","\n","    return kwargs\n","\n","class LanderQNetwork(nn.Module):\n","    def __init__(self, ob_dim, num_actions):\n","        super(LanderQNetwork, self).__init__()\n","\n","        self.fc1 = nn.Linear(ob_dim, 64)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(64, 64)\n","        self.fc_out = nn.Linear(64, num_actions)\n","\n","    def forward(self, state):\n","        y1 = self.relu(self.fc1(state))\n","        y2 = self.relu(self.fc2(y1))\n","\n","        return self.fc_out(y2)\n","\n","class AtariQNetwork(nn.Module):\n","    def __init__(self, ob_dim, num_actions):\n","        super(AtariQNetwork, self).__init__()\n","\n","        self.prec = PreprocessAtari()\n","        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n","        self.flat = Flatten()\n","        self.fc1 = nn.Linear(3136, 512)  # 3136 hard-coded based on img size + CNN layers\n","        self.fc_out = nn.Linear(512, num_actions)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, state):\n","        x = self.relu(self.conv1(self.prec(state)))\n","        x = self.relu(self.conv2(x))\n","        x = self.flat(self.relu(self.conv3(x)))\n","        x = self.relu(self.fc1(x))\n","        return self.fc_out(x)\n","\n","def create_lander_q_network(ob_dim, num_actions):\n","    return LanderQNetwork(ob_dim, num_actions)\n","\n","def create_atari_q_network(ob_dim, num_actions):\n","    return AtariQNetwork(ob_dim, num_actions)\n","\n","\n","def lander_exploration_schedule(num_timesteps):\n","    return PiecewiseSchedule(\n","        [\n","            (0, 1),\n","            (num_timesteps * 0.1, 0.02),\n","        ], outside_value=0.02\n","    )\n","\n","def atari_exploration_schedule(num_timesteps):\n","    return PiecewiseSchedule(\n","        [\n","            (0, 1.0),\n","            (num_timesteps / 16, 0.1),\n","            (num_timesteps / 8, 0.01),\n","        ], outside_value=0.01\n","    )\n","\n","def lander_optimizer():\n","    return OptimizerSpec(\n","        constructor=optim.Adam,\n","        optim_kwargs=dict(\n","            lr=1,\n","        ),\n","        learning_rate_schedule=lambda epoch: 1e-3,  # keep init learning rate\n","    )\n","\n","def atari_optimizer(num_timesteps):\n","    lr_schedule = PiecewiseSchedule(\n","        [\n","            (0, 1e-1),\n","            (num_timesteps / 40, 1e-1),\n","            (num_timesteps / 8, 5e-2),\n","        ],\n","        outside_value=5e-2,\n","    )\n","\n","    return OptimizerSpec(\n","        constructor=optim.Adam,\n","        optim_kwargs=dict(\n","            lr=1e-3,\n","            eps=1e-4\n","        ),\n","        learning_rate_schedule=lambda t: lr_schedule.value(t),\n","    )\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xbsni0fwC1k8"},"source":["#@title Critic\n","#@markdown You need to code the critic `update(..)` function\n","\n","#@markdown The code inside the `if self.double_q` can be left TODO until the second part (DDQN).\n","\n","class BaseCritic(object):\n","    def update(self, ob_no, ac_na, next_ob_no, re_n, terminal_n):\n","        raise NotImplementedError\n","\n","class DQNCritic(BaseCritic):\n","\n","    def __init__(self, hparams, optimizer_spec, **kwargs):\n","        super().__init__(**kwargs)\n","        self.env_name = hparams['env_name']\n","        self.ob_dim = hparams['ob_dim']\n","\n","        if isinstance(self.ob_dim, int):\n","            self.input_shape = (self.ob_dim,)\n","        else:\n","            self.input_shape = hparams['input_shape']\n","\n","        self.ac_dim = hparams['ac_dim']\n","        self.double_q = hparams['double_q']\n","        self.grad_norm_clipping = hparams['grad_norm_clipping']\n","        self.gamma = hparams['gamma']\n","\n","        self.optimizer_spec = optimizer_spec\n","        network_initializer = hparams['q_func']\n","        self.q_net = network_initializer(self.ob_dim, self.ac_dim)\n","        self.q_net_target = network_initializer(self.ob_dim, self.ac_dim)\n","        self.optimizer = self.optimizer_spec.constructor(\n","            self.q_net.parameters(),\n","            **self.optimizer_spec.optim_kwargs\n","        )\n","        self.learning_rate_scheduler = optim.lr_scheduler.LambdaLR(\n","            self.optimizer,\n","            self.optimizer_spec.learning_rate_schedule,\n","        )\n","        self.loss = nn.SmoothL1Loss()  # AKA Huber loss\n","        self.q_net.to(ptu.device)\n","        self.q_net_target.to(ptu.device)\n","\n","    def update(self, ob_no, ac_na, next_ob_no, reward_n, terminal_n):\n","        \"\"\"\n","            Update the parameters of the critic.\n","            let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n","                Agent.sample_trajectories\n","            let num_paths be the number of paths sampled from Agent.sample_trajectories\n","            arguments:\n","                ob_no: shape: (sum_of_path_lengths, ob_dim)\n","                next_ob_no: shape: (sum_of_path_lengths, ob_dim). The observation after taking one step forward\n","                reward_n: length: sum_of_path_lengths. Each element in reward_n is a scalar containing\n","                    the reward for each timestep\n","                terminal_n: length: sum_of_path_lengths. Each element in terminal_n is either 1 if the episode ended\n","                    at that timestep of 0 if the episode did not end\n","            returns:\n","                nothing\n","        \"\"\"\n","        ob_no = ptu.from_numpy(ob_no)\n","        ac_na = ptu.from_numpy(ac_na).to(torch.long)\n","        next_ob_no = ptu.from_numpy(next_ob_no)\n","        reward_n = ptu.from_numpy(reward_n)\n","        terminal_n = ptu.from_numpy(terminal_n)\n","\n","        qa_t_values = self.q_net(ob_no)\n","        q_t_values = torch.gather(qa_t_values, 1, ac_na.unsqueeze(1)).squeeze(1)\n","\n","        # TODO compute the Q-values from the target network\n","        qa_tp1_values = TODO\n","\n","        if self.double_q:\n","            # You must fill this part for Q2 of the Q-learning portion of the homework.\n","            # In double Q-learning, the best action is selected using the Q-network that\n","            # is being updated, but the Q-value for this action is obtained from the\n","            # target Q-network. See page 5 of https://arxiv.org/pdf/1509.06461.pdf for more details.\n","            TODO\n","        else:\n","            q_tp1, _ = qa_tp1_values.max(dim=1)\n","\n","        # compute targets for minimizing Bellman error\n","        # HINT: as you saw in lecture, this would be:\n","        #currentReward + self.gamma * qValuesOfNextTimestep * (not terminal)\n","        target = TODO\n","        target = target.detach()\n","\n","        assert q_t_values.shape == target.shape\n","        loss = self.loss(q_t_values, target)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        utils.clip_grad_value_(self.q_net.parameters(), self.grad_norm_clipping)\n","        self.optimizer.step()\n","\n","        return {\n","            'Training Loss': ptu.to_numpy(loss),\n","        }\n","\n","    def update_target_network(self):\n","        for target_param, param in zip(\n","                self.q_net_target.parameters(), self.q_net.parameters()\n","        ):\n","            target_param.data.copy_(param.data)\n","\n","    def qa_values(self, obs) -> np.ndarray:\n","        obs = ptu.from_numpy(obs)\n","        qa_values = self.q_net(obs)\n","        return ptu.to_numpy(qa_values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pri5XwNyDp9J"},"source":["#@title Policy (actor)\n","#@markdown You need to code the policy `get_action(..)` function\n","\n","class ArgMaxPolicy(object):\n","\n","    def __init__(self, critic):\n","        self.critic = critic\n","\n","    def get_action(self, obs):\n","        if len(obs.shape) > 3:\n","            observation = obs\n","        else:\n","            observation = obs[None]\n","\n","        ## TODO return the action that maxinmizes the Q-value\n","        # at the current observation as the output\n","        action = TODO\n","\n","        return action.squeeze()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWqbVw-bCJbq"},"source":["#@title DQN agent\n","#@markdown We have all the ingredientes. Let's put all together.\n","#@markdown You need to code the `step_env(..)` and the `train(..)` function\n","\n","class DQNAgent(object):\n","    def __init__(self, env, agent_params):\n","\n","        self.env = env\n","        self.agent_params = agent_params\n","        self.batch_size = agent_params['batch_size']\n","        # import ipdb; ipdb.set_trace()\n","        self.last_obs = self.env.reset()\n","\n","        self.num_actions = agent_params['ac_dim']\n","        self.learning_starts = agent_params['learning_starts']\n","        self.learning_freq = agent_params['learning_freq']\n","        self.target_update_freq = agent_params['target_update_freq']\n","\n","        self.replay_buffer_idx = None\n","        self.exploration = agent_params['exploration_schedule']\n","        self.optimizer_spec = agent_params['optimizer_spec']\n","\n","        self.critic = DQNCritic(agent_params, self.optimizer_spec)\n","        self.actor = ArgMaxPolicy(self.critic)\n","\n","        lander = agent_params['env_name'].startswith('LunarLander')\n","        self.replay_buffer = MemoryOptimizedReplayBuffer(\n","            agent_params['replay_buffer_size'], agent_params['frame_history_len'], lander=lander)\n","        self.t = 0\n","        self.num_param_updates = 0\n","\n","    def add_to_replay_buffer(self, paths):\n","        pass\n","\n","    def step_env(self):\n","        \"\"\"\n","        Step the env and store the transition\n","        At the end of this block of code, the simulator should have been\n","        advanced one step, and the replay buffer should contain one more\n","        transition. Note that self.last_obs must always point to the new latest\n","        observation.\n","        \"\"\"\n","\n","        # TODO store the latest observation (\"frame\") into the replay buffer\n","        # HINT: the replay buffer used here is `MemoryOptimizedReplayBuffer`\n","        # in dqn_utils.py\n","        self.replay_buffer_idx = TODO\n","\n","        eps = self.exploration.value(self.t)\n","\n","        # TODO use epsilon greedy exploration when selecting action\n","        # HINT: take random action with probability eps (see np.random.random())\n","        # OR if your current step number (see self.t) is less that self.learning_starts\n","        perform_random_action = TODO\n","        if perform_random_action:\n","            # take random action\n","            action = self.env.action_space.sample()\n","        else:\n","            # HINT: Your actor will take in multiple previous observations (\"frames\") in order\n","            # to deal with the partial observability of the environment. Get the most recent\n","            # `frame_history_len` observations using functionality from the replay buffer,\n","            # and then use those observations as input to your actor.\n","            action = TODO\n","\n","        # take a step in the environment using the action from the policy\n","        self.last_obs, reward, done, info = self.env.step(action)\n","\n","        # TODO store the result of taking this action into the replay buffer\n","        # HINT1: see your replay buffer's `store_effect` function\n","        # HINT2: one of the arguments you'll need to pass in is self.replay_buffer_idx from above\n","        TODO\n","\n","        # if taking this step resulted in done, reset the env (and the\n","        # latest observation)\n","        if done:\n","            self.last_obs = self.env.reset()\n","\n","    def sample(self, batch_size):\n","        if self.replay_buffer.can_sample(self.batch_size):\n","            return self.replay_buffer.sample(batch_size)\n","        else:\n","            return [], [], [], [], []\n","\n","    def train(self, ob_no, ac_na, re_n, next_ob_no, terminal_n):\n","        log = {}\n","        if (self.t > self.learning_starts\n","                and self.t % self.learning_freq == 0\n","                and self.replay_buffer.can_sample(self.batch_size)\n","        ):\n","\n","            log = self.critic.update(ob_no, ac_na, next_ob_no, re_n, terminal_n)\n","\n","            # TODO update the target network periodically\n","            # HINT: your critic already has this functionality implemented\n","            if self.num_param_updates % self.target_update_freq == 0:\n","                TODO\n","\n","            self.num_param_updates += 1\n","\n","        self.t += 1\n","        return log\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SqKtHSIOoXb"},"source":["#@title runtime arguments\n","\n","class Args:\n","\n","  def __getitem__(self, key):\n","    return getattr(self, key)\n","\n","  def __setitem__(self, key, val):\n","    setattr(self, key, val)\n","\n","  def __contains__(self, key):\n","    return hasattr(self, key)\n","\n","  env_name = 'LunarLander-v3' #@param ['MsPacman-v0', 'LunarLander-v3', 'PongNoFrameskip-v4', 'BreakoutDeterministic-v4']\n","  ep_len = 200 #@param {type: \"integer\"}\n","\n","  #@markdown batches and steps\n","  batch_size = 32 #@param {type: \"integer\"}\n","  eval_batch_size = 1000 #@param {type: \"integer\"}\n","\n","  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n","\n","  num_critic_updates_per_agent_update = 1 #@param {type: \"integer\"}\n","\n","  #@markdown Q-learning parameters\n","  double_q = False #@param {type: \"boolean\"}\n","\n","  #@markdown system\n","  save_params = False #@param {type: \"boolean\"}\n","  no_gpu = False #@param {type: \"boolean\"}\n","  which_gpu = 0 #@param {type: \"integer\"}\n","  seed = 1 #@param {type: \"integer\"}\n","\n","  #@markdown logging\n","  ## default is to not log video so\n","  ## that logs are small enough to be\n","  ## uploaded to gradscope\n","  video_log_freq =  -1 #@param {type: \"integer\"}\n","  scalar_log_freq =  10000#@param {type: \"integer\"}\n","\n","\n","args = Args()\n","\n","## ensure compatibility with hw1 code\n","args['train_batch_size'] = args['batch_size']\n","\n","\n","if args['video_log_freq'] > 0:\n","  import warnings\n","  warnings.warn(\n","      '''\\nLogging videos will make eventfiles too large.'''\n","      '''\\nSet video_log_freq = -1 to avoid that.''')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZW8OEuMOwC7","cellView":"form"},"source":["#@title Define Q-function trainer\n","#@markdown This calls the RL_trainer with the specific parameters for DQN\n","\n","class Q_Trainer(object):\n","\n","    def __init__(self, params):\n","        self.params = params\n","\n","        train_args = {\n","            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n","            'num_critic_updates_per_agent_update': params['num_critic_updates_per_agent_update'],\n","            'train_batch_size': params['batch_size'],\n","            'double_q': params['double_q'],\n","        }\n","\n","        env_args = get_env_kwargs(params['env_name'])\n","\n","        for k, v in env_args.items():\n","          params[k] = v\n","\n","        self.params['agent_class'] = DQNAgent\n","        self.params['agent_params'] = params\n","        self.params['train_batch_size'] = params['batch_size']\n","        self.params['env_wrappers'] = env_args['env_wrappers']\n","\n","        self.rl_trainer = RL_Trainer(self.params)\n","\n","    def run_training_loop(self):\n","        self.rl_trainer.run_training_loop(\n","            self.params['num_timesteps'],\n","            collect_policy = self.rl_trainer.agent.actor,\n","            eval_policy = self.rl_trainer.agent.actor,\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_i_umdGO5dp","cellView":"form"},"source":["#@title create directories for logging\n","\n","data_path = '''/content/data'''\n","\n","if not (os.path.exists(data_path)):\n","    os.makedirs(data_path)\n","\n","logdir = 'dqn_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n","logdir = os.path.join(data_path, logdir)\n","args['logdir'] = logdir\n","if not(os.path.exists(logdir)):\n","    os.makedirs(logdir)\n","\n","print(\"LOGGING TO: \", logdir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVEA-Ub1PIM6","cellView":"form"},"source":["#@markdown You can visualize your runs with tensorboard from within the notebook\n","\n","## requires tensorflow==2.3.0\n","%load_ext tensorboard\n","%tensorboard --logdir /content/data/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_EsX40XPEM3","collapsed":true},"source":["#@title run training\n","trainer = Q_Trainer(args)\n","trainer.run_training_loop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mn8j7A2sJu9"},"source":["#@title Visualize a test run on video\n","#@markdown You can run the cell multiple times to get different random initializations\n","from cs285.infrastructure.atari_wrappers import wrap_deepmind\n","\n","env = gym.make(args['env_name'], render_mode=\"rgb_array\")\n","\n","if args['env_name'] != 'LunarLander-v3':\n","    # This is only for Atari games\n","    env = wrap_deepmind(env)\n","\n","env = wrap_env(env)\n","\n","obs = env.reset()\n","\n","if args['env_name'] != 'LunarLander-v3':\n","    # This is only for Atari games\n","    frames = []\n","    for _ in range(trainer.params['frame_history_len']):\n","        frames.append(obs)\n","    npframes = np.concatenate(frames, 2)\n","else:\n","    npframes = obs\n","\n","term = False\n","i = 0\n","while not term:\n","    i += 1\n","    env.render()\n","    action= trainer.rl_trainer.agent.actor.get_action(npframes)\n","    obs, rew, term, _ = env.step(action)\n","    if args['env_name'] != 'LunarLander-v3':\n","        # This is only for Atari games\n","        frames.pop(0)\n","        frames.append(obs)\n","        npframes = np.concatenate(frames, 2)\n","    else:\n","        npframes = obs\n","\n","    if term:\n","      break;\n","\n","env.close()\n","print('Loading video...',i)\n","show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"azwHfRrp28CY","cellView":"form"},"source":["#@title Code to download the data folder.\n","#@markdown Make sure to run it frequently in case Google decides to shut down the instance suddently.\n","!zip -r /content/data.zip /content/data\n","\n","from google.colab import files\n","files.download(\"/content/data.zip\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uC_98JL5lzJq"},"source":["#@title Code to plot different metrics\n","#@markdown This is just an example of the things you can do.\n","#@markdown You might need to change the tag selected, the labels,\n","#@markdown the file names, etc.\n","\n","#@markdown **IMPORTANT:** If you run the same experiment multiple times, this will\n","#@markdown also plot error bars, but remmember to **change the seed** of the random\n","#@markdown number generator.\n","\n","#@markdown You can also run this cell in a separate colab where you upload the\n","#@markdown data folder (see https://colab.research.google.com/notebooks/io.ipynb#scrollTo=BaCkyg5CV5jF)\n","\n","# Plotting example requires tensorflow==1.12.0\n","\n","import glob\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def get_section_results(files, tag):\n","    data = []\n","    for file in files:\n","        row = []\n","        for e in tf.compat.v1.train.summary_iterator(file):\n","            for v in e.summary.value:\n","                if v.tag == tag:\n","                    row.append(v.simple_value)\n","        data.append(row)\n","    return data\n","\n","\n","#logfile = 'data/my_experiment/events*'\n","all_logdir= data_path+'/dqn_' + args.env_name + '_*'\n","logfile = all_logdir+'/events*'\n","eventfiles = glob.glob(logfile)\n","\n","tag = 'Train_AverageReturn'\n","X = get_section_results(eventfiles, tag)\n","for j, row in enumerate(X):\n","    for i, x in enumerate(row):\n","        print('Experiment {:d} | Iteration {:d} | {}: {} '.format(j, i, tag, x))\n","\n","color = 'r'\n","X = np.array(X)\n","mean_plot = X.mean(axis=0)\n","std_plot = X.std(axis=0)\n","iters = np.arange(len(mean_plot))\n","plt.plot(iters,mean_plot,color, label=tag)\n","plt.fill_between(iters, mean_plot-std_plot, mean_plot+std_plot, color=color, alpha=0.2)\n","plt.ylabel('reward')\n","plt.xlabel('iteration')\n","plt.legend()"],"execution_count":null,"outputs":[]}]}